{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ViG_test3_variousfruit.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNLq5T+tZefW6zeR+6G0wTJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"J2wkUE2Whdny"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%%bash\n","!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n","!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n","!pip install torch-geometric\n","\n","!pip install timm"],"metadata":{"id":"Oi9BdbsyhnrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","\n","cudnn.benchmark = True\n","plt.ion()   # 대화형 모드\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"-YR76J9ChsDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","# modified from https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n","# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# All rights reserved.\n","\n","# This source code is licensed under the license found in the\n","# LICENSE file in the root directory of this source tree.\n","# --------------------------------------------------------\n","# Position embedding utils\n","# --------------------------------------------------------\n","\n","import numpy as np\n","\n","import torch\n","\n","# --------------------------------------------------------\n","# relative position embedding\n","# References: https://arxiv.org/abs/2009.13658\n","# --------------------------------------------------------\n","def get_2d_relative_pos_embed(embed_dim, grid_size):\n","    \"\"\"\n","    grid_size: int of the grid height and width\n","    return:\n","    pos_embed: [grid_size*grid_size, grid_size*grid_size]\n","    \"\"\"\n","    pos_embed = get_2d_sincos_pos_embed(embed_dim, grid_size)\n","    relative_pos = 2 * np.matmul(pos_embed, pos_embed.transpose()) / pos_embed.shape[1]\n","    return relative_pos\n","\n","\n","# --------------------------------------------------------\n","# 2D sine-cosine position embedding\n","# References:\n","# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n","# MoCo v3: https://github.com/facebookresearch/moco-v3\n","# --------------------------------------------------------\n","def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n","    \"\"\"\n","    grid_size: int of the grid height and width\n","    return:\n","    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n","    \"\"\"\n","    grid_h = np.arange(grid_size, dtype=np.float32)\n","    grid_w = np.arange(grid_size, dtype=np.float32)\n","    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n","    grid = np.stack(grid, axis=0)\n","\n","    grid = grid.reshape([2, 1, grid_size, grid_size])\n","    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n","    if cls_token:\n","        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n","    return pos_embed\n","\n","\n","def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n","    assert embed_dim % 2 == 0\n","\n","    # use half of dimensions to encode grid_h\n","    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n","    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n","\n","    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n","    return emb\n","\n","\n","def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n","    \"\"\"\n","    embed_dim: output dimension for each position\n","    pos: a list of positions to be encoded: size (M,)\n","    out: (M, D)\n","    \"\"\"\n","    assert embed_dim % 2 == 0\n","    omega = np.arange(embed_dim // 2, dtype=np.float)\n","    omega /= embed_dim / 2.\n","    omega = 1. / 10000**omega  # (D/2,)\n","\n","    pos = pos.reshape(-1)  # (M,)\n","    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n","\n","    emb_sin = np.sin(out) # (M, D/2)\n","    emb_cos = np.cos(out) # (M, D/2)\n","\n","    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n","    return emb"],"metadata":{"id":"hHmf7E2uhwXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","import math\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","\n","def pairwise_distance(x):\n","    \"\"\"\n","    Compute pairwise distance of a point cloud.\n","    Args:\n","        x: tensor (batch_size, num_points, num_dims)\n","    Returns:\n","        pairwise distance: (batch_size, num_points, num_points)\n","    \"\"\"\n","    with torch.no_grad():\n","        x_inner = -2*torch.matmul(x, x.transpose(2, 1))\n","        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n","        return x_square + x_inner + x_square.transpose(2, 1)\n","\n","\n","def part_pairwise_distance(x, start_idx=0, end_idx=1):\n","    \"\"\"\n","    Compute pairwise distance of a point cloud.\n","    Args:\n","        x: tensor (batch_size, num_points, num_dims)\n","    Returns:\n","        pairwise distance: (batch_size, num_points, num_points)\n","    \"\"\"\n","    with torch.no_grad():\n","        x_part = x[:, start_idx:end_idx]\n","        x_square_part = torch.sum(torch.mul(x_part, x_part), dim=-1, keepdim=True)\n","        x_inner = -2*torch.matmul(x_part, x.transpose(2, 1))\n","        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n","        return x_square_part + x_inner + x_square.transpose(2, 1)\n","\n","\n","def xy_pairwise_distance(x, y):\n","    \"\"\"\n","    Compute pairwise distance of a point cloud.\n","    Args:\n","        x: tensor (batch_size, num_points, num_dims)\n","    Returns:\n","        pairwise distance: (batch_size, num_points, num_points)\n","    \"\"\"\n","    with torch.no_grad():\n","        xy_inner = -2*torch.matmul(x, y.transpose(2, 1))\n","        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n","        y_square = torch.sum(torch.mul(y, y), dim=-1, keepdim=True)\n","        return x_square + xy_inner + y_square.transpose(2, 1)\n","\n","\n","def dense_knn_matrix(x, k=16, relative_pos=None):\n","    \"\"\"Get KNN based on the pairwise distance.\n","    Args:\n","        x: (batch_size, num_dims, num_points, 1)\n","        k: int\n","    Returns:\n","        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n","    \"\"\"\n","    with torch.no_grad():\n","        x = x.transpose(2, 1).squeeze(-1)\n","        batch_size, n_points, n_dims = x.shape\n","        ### memory efficient implementation ###\n","        n_part = 10000\n","        if n_points > n_part:\n","            nn_idx_list = []\n","            groups = math.ceil(n_points / n_part)\n","            for i in range(groups):\n","                start_idx = n_part * i\n","                end_idx = min(n_points, n_part * (i + 1))\n","                dist = part_pairwise_distance(x.detach(), start_idx, end_idx)\n","                if relative_pos is not None:\n","                    dist += relative_pos[:, start_idx:end_idx]\n","                _, nn_idx_part = torch.topk(-dist, k=k)\n","                nn_idx_list += [nn_idx_part]\n","            nn_idx = torch.cat(nn_idx_list, dim=1)\n","        else:\n","            dist = pairwise_distance(x.detach())\n","            if relative_pos is not None:\n","                dist += relative_pos\n","            _, nn_idx = torch.topk(-dist, k=k) # b, n, k\n","        ######\n","        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n","    return torch.stack((nn_idx, center_idx), dim=0)\n","\n","\n","def xy_dense_knn_matrix(x, y, k=16, relative_pos=None):\n","    \"\"\"Get KNN based on the pairwise distance.\n","    Args:\n","        x: (batch_size, num_dims, num_points, 1)\n","        k: int\n","    Returns:\n","        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n","    \"\"\"\n","    with torch.no_grad():\n","        x = x.transpose(2, 1).squeeze(-1)\n","        y = y.transpose(2, 1).squeeze(-1)\n","        batch_size, n_points, n_dims = x.shape\n","        dist = xy_pairwise_distance(x.detach(), y.detach())\n","        if relative_pos is not None:\n","            dist += relative_pos\n","        _, nn_idx = torch.topk(-dist, k=k)\n","        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n","    return torch.stack((nn_idx, center_idx), dim=0)\n","\n","\n","class DenseDilated(nn.Module):\n","    \"\"\"\n","    Find dilated neighbor from neighbor list\n","\n","    edge_index: (2, batch_size, num_points, k)\n","    \"\"\"\n","    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n","        super(DenseDilated, self).__init__()\n","        self.dilation = dilation\n","        self.stochastic = stochastic\n","        self.epsilon = epsilon\n","        self.k = k\n","\n","    def forward(self, edge_index):\n","        if self.stochastic:\n","            if torch.rand(1) < self.epsilon and self.training:\n","                num = self.k * self.dilation\n","                randnum = torch.randperm(num)[:self.k]\n","                edge_index = edge_index[:, :, :, randnum]\n","            else:\n","                edge_index = edge_index[:, :, :, ::self.dilation]\n","        else:\n","            edge_index = edge_index[:, :, :, ::self.dilation]\n","        return edge_index\n","\n","\n","class DenseDilatedKnnGraph(nn.Module):\n","    \"\"\"\n","    Find the neighbors' indices based on dilated knn\n","    \"\"\"\n","    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n","        super(DenseDilatedKnnGraph, self).__init__()\n","        self.dilation = dilation\n","        self.stochastic = stochastic\n","        self.epsilon = epsilon\n","        self.k = k\n","        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n","\n","    def forward(self, x, y=None, relative_pos=None):\n","        if y is not None:\n","            #### normalize\n","            x = F.normalize(x, p=2.0, dim=1)\n","            y = F.normalize(y, p=2.0, dim=1)\n","            ####\n","            edge_index = xy_dense_knn_matrix(x, y, self.k * self.dilation, relative_pos)\n","        else:\n","            #### normalize\n","            x = F.normalize(x, p=2.0, dim=1)\n","            ####\n","            edge_index = dense_knn_matrix(x, self.k * self.dilation, relative_pos)\n","        return self._dilated(edge_index)"],"metadata":{"id":"8sAabum0h1gD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","import torch\n","from torch import nn\n","from torch.nn import Sequential as Seq, Linear as Lin, Conv2d\n","\n","\n","##############################\n","#    Basic layers\n","##############################\n","def act_layer(act, inplace=False, neg_slope=0.2, n_prelu=1):\n","    # activation layer\n","\n","    act = act.lower()\n","    if act == 'relu':\n","        layer = nn.ReLU(inplace)\n","    elif act == 'leakyrelu':\n","        layer = nn.LeakyReLU(neg_slope, inplace)\n","    elif act == 'prelu':\n","        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n","    elif act == 'gelu':\n","        layer = nn.GELU()\n","    elif act == 'hswish':\n","        layer = nn.Hardswish(inplace)\n","    else:\n","        raise NotImplementedError('activation layer [%s] is not found' % act)\n","    return layer\n","\n","\n","def norm_layer(norm, nc):\n","    # normalization layer 2d\n","    norm = norm.lower()\n","    if norm == 'batch':\n","        layer = nn.BatchNorm2d(nc, affine=True)\n","    elif norm == 'instance':\n","        layer = nn.InstanceNorm2d(nc, affine=False)\n","    else:\n","        raise NotImplementedError('normalization layer [%s] is not found' % norm)\n","    return layer\n","\n","\n","class MLP(Seq):\n","    def __init__(self, channels, act='relu', norm=None, bias=True):\n","        m = []\n","        for i in range(1, len(channels)):\n","            m.append(Lin(channels[i - 1], channels[i], bias))\n","            if act is not None and act.lower() != 'none':\n","                m.append(act_layer(act))\n","            if norm is not None and norm.lower() != 'none':\n","                m.append(norm_layer(norm, channels[-1]))\n","        super(MLP, self).__init__(*m)\n","\n","\n","class BasicConv(Seq):\n","    def __init__(self, channels, act='relu', norm=None, bias=True, drop=0.):\n","        m = []\n","        for i in range(1, len(channels)):\n","            m.append(Conv2d(channels[i - 1], channels[i], 1, bias=bias, groups=4))\n","            if norm is not None and norm.lower() != 'none':\n","                m.append(norm_layer(norm, channels[-1]))\n","            if act is not None and act.lower() != 'none':\n","                m.append(act_layer(act))\n","            if drop > 0:\n","                m.append(nn.Dropout2d(drop))\n","\n","        super(BasicConv, self).__init__(*m)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","\n","def batched_index_select(x, idx):\n","    r\"\"\"fetches neighbors features from a given neighbor idx\n","\n","    Args:\n","        x (Tensor): input feature Tensor\n","                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times 1}`.\n","        idx (Tensor): edge_idx\n","                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times l}`.\n","    Returns:\n","        Tensor: output neighbors features\n","            :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times k}`.\n","    \"\"\"\n","    batch_size, num_dims, num_vertices_reduced = x.shape[:3]\n","    _, num_vertices, k = idx.shape\n","    idx_base = torch.arange(0, batch_size, device=idx.device).view(-1, 1, 1) * num_vertices_reduced\n","    idx = idx + idx_base\n","    idx = idx.contiguous().view(-1)\n","\n","    x = x.transpose(2, 1)\n","    feature = x.contiguous().view(batch_size * num_vertices_reduced, -1)[idx, :]\n","    feature = feature.view(batch_size, num_vertices, k, num_dims).permute(0, 3, 1, 2).contiguous()\n","    return feature"],"metadata":{"id":"iBLx_UJXh3Oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","import numpy as np\n","import torch\n","from torch import nn\n","# from .torch_nn import BasicConv, batched_index_select, act_layer\n","# from .torch_edge import DenseDilatedKnnGraph\n","# from .pos_embed import get_2d_relative_pos_embed\n","import torch.nn.functional as F\n","from timm.models.layers import DropPath\n","\n","\n","class MRConv2d(nn.Module):\n","    \"\"\"\n","    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(MRConv2d, self).__init__()\n","        self.nn = BasicConv([in_channels*2, out_channels], act, norm, bias)\n","\n","    def forward(self, x, edge_index, y=None):\n","        x_i = batched_index_select(x, edge_index[1])\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)\n","        b, c, n, _ = x.shape\n","        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2)], dim=2).reshape(b, 2 * c, n, _)\n","        return self.nn(x)\n","\n","\n","class EdgeConv2d(nn.Module):\n","    \"\"\"\n","    Edge convolution layer (with activation, batch normalization) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(EdgeConv2d, self).__init__()\n","        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)\n","\n","    def forward(self, x, edge_index, y=None):\n","        x_i = batched_index_select(x, edge_index[1])\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)\n","        return max_value\n","\n","\n","class GraphSAGE(nn.Module):\n","    \"\"\"\n","    GraphSAGE Graph Convolution (Paper: https://arxiv.org/abs/1706.02216) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(GraphSAGE, self).__init__()\n","        self.nn1 = BasicConv([in_channels, in_channels], act, norm, bias)\n","        self.nn2 = BasicConv([in_channels*2, out_channels], act, norm, bias)\n","\n","    def forward(self, x, edge_index, y=None):\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        x_j, _ = torch.max(self.nn1(x_j), -1, keepdim=True)\n","        return self.nn2(torch.cat([x, x_j], dim=1))\n","\n","\n","class GINConv2d(nn.Module):\n","    \"\"\"\n","    GIN Graph Convolution (Paper: https://arxiv.org/abs/1810.00826) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(GINConv2d, self).__init__()\n","        self.nn = BasicConv([in_channels, out_channels], act, norm, bias)\n","        eps_init = 0.0\n","        self.eps = nn.Parameter(torch.Tensor([eps_init]))\n","\n","    def forward(self, x, edge_index, y=None):\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        x_j = torch.sum(x_j, -1, keepdim=True)\n","        return self.nn((1 + self.eps) * x + x_j)\n","\n","\n","class GraphConv2d(nn.Module):\n","    \"\"\"\n","    Static graph convolution layer\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, conv='edge', act='relu', norm=None, bias=True):\n","        super(GraphConv2d, self).__init__()\n","        if conv == 'edge':\n","            self.gconv = EdgeConv2d(in_channels, out_channels, act, norm, bias)\n","        elif conv == 'mr':\n","            self.gconv = MRConv2d(in_channels, out_channels, act, norm, bias)\n","        elif conv == 'sage':\n","            self.gconv = GraphSAGE(in_channels, out_channels, act, norm, bias)\n","        elif conv == 'gin':\n","            self.gconv = GINConv2d(in_channels, out_channels, act, norm, bias)\n","        else:\n","            raise NotImplementedError('conv:{} is not supported'.format(conv))\n","\n","    def forward(self, x, edge_index, y=None):\n","        return self.gconv(x, edge_index, y)\n","\n","\n","class DyGraphConv2d(GraphConv2d):\n","    \"\"\"\n","    Dynamic graph convolution layer\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=9, dilation=1, conv='edge', act='relu',\n","                 norm=None, bias=True, stochastic=False, epsilon=0.0, r=1):\n","        super(DyGraphConv2d, self).__init__(in_channels, out_channels, conv, act, norm, bias)\n","        self.k = kernel_size\n","        self.d = dilation\n","        self.r = r\n","        self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n","\n","    def forward(self, x, relative_pos=None):\n","        B, C, H, W = x.shape\n","        y = None\n","        if self.r > 1:\n","            y = F.avg_pool2d(x, self.r, self.r)\n","            y = y.reshape(B, C, -1, 1).contiguous()            \n","        x = x.reshape(B, C, -1, 1).contiguous()\n","        edge_index = self.dilated_knn_graph(x, y, relative_pos)\n","        x = super(DyGraphConv2d, self).forward(x, edge_index, y)\n","        return x.reshape(B, -1, H, W).contiguous()\n","\n","\n","class Grapher(nn.Module):\n","    \"\"\"\n","    Grapher module with graph convolution and fc layers\n","    \"\"\"\n","    def __init__(self, in_channels, kernel_size=9, dilation=1, conv='edge', act='relu', norm=None,\n","                 bias=True,  stochastic=False, epsilon=0.0, r=1, n=196, drop_path=0.0, relative_pos=False):\n","        super(Grapher, self).__init__()\n","        self.channels = in_channels\n","        self.n = n\n","        self.r = r\n","        self.fc1 = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(in_channels),\n","        )\n","        self.graph_conv = DyGraphConv2d(in_channels, in_channels * 2, kernel_size, dilation, conv,\n","                              act, norm, bias, stochastic, epsilon, r)\n","        self.fc2 = nn.Sequential(\n","            nn.Conv2d(in_channels * 2, in_channels, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(in_channels),\n","        )\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.relative_pos = None\n","        if relative_pos:\n","            print('using relative_pos')\n","            relative_pos_tensor = torch.from_numpy(np.float32(get_2d_relative_pos_embed(in_channels,\n","                int(n**0.5)))).unsqueeze(0).unsqueeze(1)\n","            relative_pos_tensor = F.interpolate(\n","                    relative_pos_tensor, size=(n, n//(r*r)), mode='bicubic', align_corners=False)\n","            self.relative_pos = nn.Parameter(-relative_pos_tensor.squeeze(1), requires_grad=False)\n","\n","    def _get_relative_pos(self, relative_pos, H, W):\n","        if relative_pos is None or H * W == self.n:\n","            return relative_pos\n","        else:\n","            N = H * W\n","            N_reduced = N // (self.r * self.r)\n","            return F.interpolate(relative_pos.unsqueeze(0), size=(N, N_reduced), mode=\"bicubic\").squeeze(0)\n","\n","    def forward(self, x):\n","        _tmp = x\n","        x = self.fc1(x)\n","        B, C, H, W = x.shape\n","        relative_pos = self._get_relative_pos(self.relative_pos, H, W)\n","        x = self.graph_conv(x, relative_pos)\n","        x = self.fc2(x)\n","        x = self.drop_path(x) + _tmp\n","        return x"],"metadata":{"id":"7mI9IOgsh4pG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential as Seq\n","\n","from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","from timm.models.helpers import load_pretrained\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.registry import register_model\n","\n","# from gcn_lib import Grapher, act_layer\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n","        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n","        **kwargs\n","    }\n","\n","\n","default_cfgs = {\n","    'vig_224_gelu': _cfg(\n","        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n","    ),\n","    'vig_b_224_gelu': _cfg(\n","        crop_pct=0.95, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n","    ),\n","}\n","\n","\n","class FFN(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act='relu', drop_path=0.0):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Sequential(\n","            nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(hidden_features),\n","        )\n","        self.act = act_layer(act)\n","        self.fc2 = nn.Sequential(\n","            nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(out_features),\n","        )\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\n","    def forward(self, x):\n","        shortcut = x\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.fc2(x)\n","        x = self.drop_path(x) + shortcut\n","        return x#.reshape(B, C, N, 1)\n","\n","\n","class Stem(nn.Module):\n","    \"\"\" Image to Visual Embedding\n","    Overlap: https://arxiv.org/pdf/2106.13797.pdf\n","    \"\"\"\n","    def __init__(self, img_size=224, in_dim=3, out_dim=768, act='relu'):\n","        super().__init__()        \n","        self.convs = nn.Sequential(\n","            nn.Conv2d(in_dim, out_dim//2, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim//2),\n","            act_layer(act),\n","            nn.Conv2d(out_dim//2, out_dim, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","            act_layer(act),\n","            nn.Conv2d(out_dim, out_dim, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        return x\n","\n","\n","class Downsample(nn.Module):\n","    \"\"\" Convolution-based downsample\n","    \"\"\"\n","    def __init__(self, in_dim=3, out_dim=768):\n","        super().__init__()        \n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class DeepGCN(torch.nn.Module):\n","    def __init__(self, opt):\n","        super(DeepGCN, self).__init__()\n","        print(opt)\n","        k = opt.k\n","        act = opt.act\n","        norm = opt.norm\n","        bias = opt.bias\n","        epsilon = opt.epsilon\n","        stochastic = opt.use_stochastic\n","        conv = opt.conv\n","        emb_dims = opt.emb_dims\n","        drop_path = opt.drop_path\n","        \n","        blocks = opt.blocks\n","        self.n_blocks = sum(blocks)\n","        channels = opt.channels\n","        reduce_ratios = [4, 2, 1, 1]\n","        dpr = [x.item() for x in torch.linspace(0, drop_path, self.n_blocks)]  # stochastic depth decay rule \n","        num_knn = [int(x.item()) for x in torch.linspace(k, k, self.n_blocks)]  # number of knn's k\n","        max_dilation = 49 // max(num_knn)\n","        \n","        self.stem = Stem(out_dim=channels[0], act=act)\n","        self.pos_embed = nn.Parameter(torch.zeros(1, channels[0], 224//4, 224//4))\n","        HW = 224 // 4 * 224 // 4\n","\n","        self.backbone = nn.ModuleList([])\n","        idx = 0\n","        for i in range(len(blocks)):\n","            if i > 0:\n","                self.backbone.append(Downsample(channels[i-1], channels[i]))\n","                HW = HW // 4\n","            for j in range(blocks[i]):\n","                self.backbone += [\n","                    Seq(Grapher(channels[i], num_knn[idx], min(idx // 4 + 1, max_dilation), conv, act, norm,\n","                                    bias, stochastic, epsilon, reduce_ratios[i], n=HW, drop_path=dpr[idx],\n","                                    relative_pos=True),\n","                          FFN(channels[i], channels[i] * 4, act=act, drop_path=dpr[idx])\n","                         )]\n","                idx += 1\n","        self.backbone = Seq(*self.backbone)\n","\n","        self.prediction = Seq(nn.Conv2d(channels[-1], 1024, 1, bias=True),\n","                              nn.BatchNorm2d(1024),\n","                              act_layer(act),\n","                              nn.Dropout(opt.dropout),\n","                              nn.Conv2d(1024, opt.n_classes, 1, bias=True))\n","        self.model_init()\n","\n","    def model_init(self):\n","        for m in self.modules():\n","            if isinstance(m, torch.nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","                m.weight.requires_grad = True\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","                    m.bias.requires_grad = True\n","\n","    def forward(self, inputs):\n","        x = self.stem(inputs) + self.pos_embed\n","        B, C, H, W = x.shape\n","        for i in range(len(self.backbone)):\n","            x = self.backbone[i](x)\n","\n","        x = F.adaptive_avg_pool2d(x, 1)\n","        return self.prediction(x).squeeze(-1).squeeze(-1)\n","\n","\n","@register_model\n","def pvig_ti_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n","            self.channels = [48, 96, 240, 384] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_224_gelu']\n","    return model\n","\n","\n","@register_model\n","def pvig_s_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n","            self.channels = [80, 160, 400, 640] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_224_gelu']\n","    return model\n","\n","\n","@register_model\n","def pvig_m_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,16,2] # number of basic blocks in the backbone\n","            self.channels = [96, 192, 384, 768] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_224_gelu']\n","    return model\n","\n","\n","@register_model\n","def pvig_b_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,18,2] # number of basic blocks in the backbone\n","            self.channels = [128, 256, 512, 1024] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_b_224_gelu']\n","    return model"],"metadata":{"id":"oGE_MR9yh6th"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = pvig_b_224()\n","model = model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n","criterion = nn.CrossEntropyLoss()\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"],"metadata":{"id":"npaE9Jg3h8_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","\n","# input = torch.Tensor([(3, 224, 224)], device=dev)\n","summary(model, (3, 224, 224))"],"metadata":{"id":"GaQ3X2V4h-lX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습을 위해 데이터 증가(augmentation) 및 일반화(normalization)\n","# 검증을 위한 일반화\n","# from email.mime import image\n","\n","data_transforms = transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# data_transforms = {\n","#     'train': transforms.Compose([\n","#         transforms.RandomResizedCrop(224),\n","#         transforms.RandomHorizontalFlip(),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","#     ]),\n","    # 'test': transforms.Compose([\n","    #     transforms.Resize(256),\n","    #     transforms.CenterCrop(224),\n","    #     transforms.ToTensor(),\n","    #     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    # ]),\n","# }\n","\n","\n","data_dir = '/content/drive/MyDrive/Data/various_fruit'\n","image_datasets = datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms)\n","                 \n","print(len(image_datasets))\n","           \n","dataset_sizes = len(image_datasets)\n","print(dataset_sizes)\n","class_names = image_datasets.classes"],"metadata":{"id":"zBTDuZfaiAN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(image_datasets, class_names, test_size=0.2,  shuffle=True, random_state=1004)\n","print('X_train shape:', X_train.shape)\n","print('X_val shape:', X_val.shape)\n","print('y_train shape:', y_train.shape)\n","print('y_val shape:', y_val.shape)"],"metadata":{"id":"krpz8OObiprD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainloader = torch.utils.data.DataLoader(X_train, batch_size=32,shuffle=True, num_workers=2)\n","valloader = torch.utils.data.DataLoader(X_val, batch_size=32,shuffle=True, num_workers=2)  "],"metadata":{"id":"WeDDkJR2kN5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transform 된 배치 이미지 시각화\n","def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # 갱신이 될 때까지 잠시 기다립니다.\n","\n","\n","# 학습 데이터의 배치를 얻습니다.\n","inputs, classes = next(iter(trainloader))\n","\n","# 배치로부터 격자 형태의 이미지를 만듭니다.\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out)\n","print([class_names[x] for x in classes])"],"metadata":{"id":"Ji9xpCB8iyxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transform 된 배치 이미지 시각화\n","def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # 갱신이 될 때까지 잠시 기다립니다.\n","\n","\n","# 학습 데이터의 배치를 얻습니다.\n","inputs, classes = next(iter(valloader))\n","\n","# 배치로부터 격자 형태의 이미지를 만듭니다.\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out)\n","print([class_names[x] for x in classes])"],"metadata":{"id":"uytNWgzRj4IY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorboardX\n","!pip install tensorboard "],"metadata":{"id":"xRuWNEXBkhHR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","\n","writer = SummaryWriter()\n","\n","def train_model(model, criterion, optimizer, scheduler, num_epochs):\n","    # model = model.to(device)\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in tqdm(range(num_epochs)):\n","        \n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print('-' * 10)\n","        # print(f\"time per 1 epoch: {time.time() - since:.2f}\")\n","\n","        # 각 에폭(epoch)은 학습 단계와 검증 단계를 갖습니다.\n","        # for phase in ['train', 'test']:\n","        #     if phase == 'train':\n","                # model.load_state_dict(model_state_dict)\n","        model.train()  # 모델을 학습 모드로 설정\n","            # else:\n","                # model.load_state_dict(model_state_dict)\n","                \n","\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        # 데이터를 반복\n","        for inputs, labels in trainloader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # 매개변수 경사도를 0으로 설정\n","            optimizer.zero_grad()\n","\n","            # 순전파\n","            # 학습 시에만 연산 기록을 추적\n","            with torch.set_grad_enabled(True):\n","                outputs = model(inputs)\n","                _, preds = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","\n","                # 학습 단계인 경우 역전파 + 최적화\n","                # if phase == 'train':\n","                loss.backward()\n","                optimizer.step()\n","\n","                # 통계\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            # if phase == 'train':\n","            scheduler.step()\n","\n","            epoch_train_loss = running_loss / len(X_train)\n","            epoch_train_acc = running_corrects.double() / len(X_train)\n","\n","            print(f'train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n","            writer.add_scalar(\"Loss/train\", epoch_train_loss, epoch)\n","            writer.add_scalar(\"Acc/train\", epoch_train_acc, epoch)\n","            # wandb.log({'train_acc': epoch_acc, 'train_loss' : epoch_loss})\n","          \n","          model.eval()\n","          \n","          # 모델을 깊은 복사(deep copy)함\n","            for inputs, labels in valloader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # 매개변수 경사도를 0으로 설정\n","            optimizer.zero_grad()\n","\n","            # 순전파\n","            # 학습 시에만 연산 기록을 추적\n","            with torch.set_grad_enabled(False):\n","                outputs = model(inputs)\n","                _, preds = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","\n","                # 학습 단계인 경우 역전파 + 최적화\n","                # if phase == 'train':\n","                loss.backward()\n","                optimizer.step()\n","\n","                # 통계\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            # if phase == 'train':\n","            scheduler.step()\n","\n","            epoch_val_loss = running_loss / len(X_val)\n","            epoch_val_acc = running_corrects.double() / len(X_val)\n","\n","            print(f'train Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n","            writer.add_scalar(\"Loss/val\", epoch_val_loss, epoch)\n","            writer.add_scalar(\"Acc/val\", epoch_val_acc, epoch)\n","            \n","            if epoch_val_acc > best_acc:\n","                   # 모델을 평가 모드로 설정\n","                best_acc = epoch_val_acc\n","                print(\"Update best acc! :\", best_acc)\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","        time.sleep(0.1)\n","\n","    time_elapsed = time.time() - since\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best val Acc: {best_acc:4f}')\n","    # wandb.log({'test_acc': best_acc})\n","    \n","\n","    # 가장 나은 모델 가중치를 불러옴\n","    model.load_state_dict(best_model_wts)\n","    return model"],"metadata":{"id":"zHk6m_eqkmL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","# # from tqdm.notebook import trange, tqdm\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","train_model(model, criterion, optimizer, exp_lr_scheduler,\n","                       num_epochs=20)\n"],"metadata":{"id":"mvv5uZutknQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = ViG-Ti\n","model_ti = pvig_ti_224()\n","model_ti = model_ti.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n","criterion = nn.CrossEntropyLoss()\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","trainloader = torch.utils.data.DataLoader(X_train, batch_size=64,shuffle=True, num_workers=2)\n","valloader = torch.utils.data.DataLoader(X_val, batch_size=64,shuffle=True, num_workers=2)  \n","\n","train_model(model, criterion, optimizer, exp_lr_scheduler,\n","                       num_epochs=20)"],"metadata":{"id":"QomiDJKKnYwv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = ViG-S\n","import gc\n","# # from tqdm.notebook import trange, tqdm\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","model_ti = pvig_s_224()\n","model_ti = model_ti.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n","criterion = nn.CrossEntropyLoss()\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","trainloader = torch.utils.data.DataLoader(X_train, batch_size=32,shuffle=True, num_workers=2)\n","valloader = torch.utils.data.DataLoader(X_val, batch_size=32,shuffle=True, num_workers=2)  \n","\n","train_model(model, criterion, optimizer, exp_lr_scheduler,\n","                       num_epochs=20)"],"metadata":{"id":"jZKs4mKsnn4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = ViG-M\n","import gc\n","# # from tqdm.notebook import trange, tqdm\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","model_ti = pvig_m_224()\n","model_ti = model_ti.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n","criterion = nn.CrossEntropyLoss()\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","trainloader = torch.utils.data.DataLoader(X_train, batch_size=32,shuffle=True, num_workers=2)\n","valloader = torch.utils.data.DataLoader(X_val, batch_size=32,shuffle=True, num_workers=2) \n","\n","train_model(model, criterion, optimizer, exp_lr_scheduler,\n","                       num_epochs=20)"],"metadata":{"id":"WvYnoRo-n5vN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tensorboard --logdir runs"],"metadata":{"id":"7Rp4QNdKoCfz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pretrained CNN  \n","### Compared to resnet18 model"],"metadata":{"id":"lRnr0TxgogJF"}},{"cell_type":"code","source":["# 미리 학습된 resnet 모델 불러오기\n","model_ft = models.resnet18(pretrained=True)\n","model_ft = model_ft.to(device)\n","\n","# 마지막 연결계층의 입력부분까지 받아오기 > 완전계층 이후 부분은 학습\n","num_ftrs = model_ft.fc.in_features\n","# 여기서 각 출력 샘플의 크기는 2로 설정합니다.\n","# 또는, nn.Linear(num_ftrs, len (class_names))로 일반화할 수 있습니다.\n","model_ft.fc = nn.Linear(num_ftrs, 36)\n","model_ft.fc = model_ft.fc.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# 모든 매개변수들이 최적화되었는지 관찰\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# 7 에폭마다 0.1씩 학습률 감소\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"metadata":{"id":"gvD-0O-TofCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model_ft, (3,224,224))"],"metadata":{"id":"HQoXOuDcoicd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=20)"],"metadata":{"id":"7gKZRffWojuc"},"execution_count":null,"outputs":[]}]}