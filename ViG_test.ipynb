{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ViG_test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNVmuU1eAEJBonT6LDXy/DP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"m36enHHnus_D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660398570263,"user_tz":-540,"elapsed":17064,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}},"outputId":"642f55a8-7d32-4e1d-d256-d3126114f929"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!python -c \"import torch; print(torch.__version__)\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvZPB0AfT3Wb","executionInfo":{"status":"ok","timestamp":1660392340055,"user_tz":-540,"elapsed":8,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}},"outputId":"e9840a74-d05c-4566-efa9-11c4d29ee049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n"]}]},{"cell_type":"code","source":["!python -c \"import torch; print(torch.version.cuda)\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKdipWqHT4wu","executionInfo":{"status":"ok","timestamp":1660398594563,"user_tz":-540,"elapsed":3589,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}},"outputId":"c65928f6-3862-4a6b-bb1c-fad7d0f64327"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["11.3\n"]}]},{"cell_type":"code","source":["%%bash\n","pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n","pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n","pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n","pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html"],"metadata":{"id":"08ZDP4lXT8G5","executionInfo":{"status":"ok","timestamp":1660398713861,"user_tz":-540,"elapsed":52786,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"39692a55-4488-4a1d-faee-5b1fe3deb7ad"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Process is terminated.\n"]}]},{"cell_type":"code","source":["pip install torch-geometric"],"metadata":{"id":"ObF_HPJgbZLu","executionInfo":{"status":"ok","timestamp":1660398653330,"user_tz":-540,"elapsed":3766,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"78352164-ef50-4cb8-943c-9e1c51ba8845"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n","\u001b[K     |████████████████████████████████| 407 kB 28.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"]}]},{"cell_type":"code","source":["pip install timm"],"metadata":{"id":"AL5VAuldvgaV","executionInfo":{"status":"ok","timestamp":1660398655935,"user_tz":-540,"elapsed":2609,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"769c86c1-2669-4a73-f59f-ca5163c4bb95"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n","\u001b[K     |████████████████████████████████| 509 kB 31.9 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"]}]},{"cell_type":"code","source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","\n","cudnn.benchmark = True\n","plt.ion()   # 대화형 모드"],"metadata":{"id":"PuWwuE--RnoC","executionInfo":{"status":"error","timestamp":1660398714322,"user_tz":-540,"elapsed":471,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}},"colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"f73fcc82-68aa-4d8e-f3bd-285f9d32b421"},"execution_count":9,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-34e9f7ae3410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# pretrained model 불러오기\n","path = '/content/drive/MyDrive/pretrained/pvig_b.pth.tar'\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_state_dict = torch.load(path, map_location=device)\n","# print(model_state_dict)"],"metadata":{"id":"Z5qOnXCXTF8r","executionInfo":{"status":"aborted","timestamp":1660398714314,"user_tz":-540,"elapsed":460,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 구현"],"metadata":{"id":"41OXbztsZnWA"}},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","# modified from https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n","# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# All rights reserved.\n","\n","# This source code is licensed under the license found in the\n","# LICENSE file in the root directory of this source tree.\n","# --------------------------------------------------------\n","# Position embedding utils\n","# --------------------------------------------------------\n","\n","import numpy as np\n","\n","import torch\n","\n","# --------------------------------------------------------\n","# relative position embedding\n","# References: https://arxiv.org/abs/2009.13658\n","# --------------------------------------------------------\n","def get_2d_relative_pos_embed(embed_dim, grid_size):\n","    \"\"\"\n","    grid_size: int of the grid height and width\n","    return:\n","    pos_embed: [grid_size*grid_size, grid_size*grid_size]\n","    \"\"\"\n","    pos_embed = get_2d_sincos_pos_embed(embed_dim, grid_size)\n","    relative_pos = 2 * np.matmul(pos_embed, pos_embed.transpose()) / pos_embed.shape[1]\n","    return relative_pos\n","\n","\n","# --------------------------------------------------------\n","# 2D sine-cosine position embedding\n","# References:\n","# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n","# MoCo v3: https://github.com/facebookresearch/moco-v3\n","# --------------------------------------------------------\n","def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n","    \"\"\"\n","    grid_size: int of the grid height and width\n","    return:\n","    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n","    \"\"\"\n","    grid_h = np.arange(grid_size, dtype=np.float32)\n","    grid_w = np.arange(grid_size, dtype=np.float32)\n","    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n","    grid = np.stack(grid, axis=0)\n","\n","    grid = grid.reshape([2, 1, grid_size, grid_size])\n","    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n","    if cls_token:\n","        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n","    return pos_embed\n","\n","\n","def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n","    assert embed_dim % 2 == 0\n","\n","    # use half of dimensions to encode grid_h\n","    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n","    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n","\n","    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n","    return emb\n","\n","\n","def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n","    \"\"\"\n","    embed_dim: output dimension for each position\n","    pos: a list of positions to be encoded: size (M,)\n","    out: (M, D)\n","    \"\"\"\n","    assert embed_dim % 2 == 0\n","    omega = np.arange(embed_dim // 2, dtype=np.float)\n","    omega /= embed_dim / 2.\n","    omega = 1. / 10000**omega  # (D/2,)\n","\n","    pos = pos.reshape(-1)  # (M,)\n","    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n","\n","    emb_sin = np.sin(out) # (M, D/2)\n","    emb_cos = np.cos(out) # (M, D/2)\n","\n","    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n","    return emb"],"metadata":{"id":"q_kixpvDvRp_","executionInfo":{"status":"aborted","timestamp":1660398714315,"user_tz":-540,"elapsed":461,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","import math\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","\n","def pairwise_distance(x):\n","    \"\"\"\n","    Compute pairwise distance of a point cloud.\n","    Args:\n","        x: tensor (batch_size, num_points, num_dims)\n","    Returns:\n","        pairwise distance: (batch_size, num_points, num_points)\n","    \"\"\"\n","    with torch.no_grad():\n","        x_inner = -2*torch.matmul(x, x.transpose(2, 1))\n","        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n","        return x_square + x_inner + x_square.transpose(2, 1)\n","\n","\n","def part_pairwise_distance(x, start_idx=0, end_idx=1):\n","    \"\"\"\n","    Compute pairwise distance of a point cloud.\n","    Args:\n","        x: tensor (batch_size, num_points, num_dims)\n","    Returns:\n","        pairwise distance: (batch_size, num_points, num_points)\n","    \"\"\"\n","    with torch.no_grad():\n","        x_part = x[:, start_idx:end_idx]\n","        x_square_part = torch.sum(torch.mul(x_part, x_part), dim=-1, keepdim=True)\n","        x_inner = -2*torch.matmul(x_part, x.transpose(2, 1))\n","        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n","        return x_square_part + x_inner + x_square.transpose(2, 1)\n","\n","\n","def xy_pairwise_distance(x, y):\n","    \"\"\"\n","    Compute pairwise distance of a point cloud.\n","    Args:\n","        x: tensor (batch_size, num_points, num_dims)\n","    Returns:\n","        pairwise distance: (batch_size, num_points, num_points)\n","    \"\"\"\n","    with torch.no_grad():\n","        xy_inner = -2*torch.matmul(x, y.transpose(2, 1))\n","        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n","        y_square = torch.sum(torch.mul(y, y), dim=-1, keepdim=True)\n","        return x_square + xy_inner + y_square.transpose(2, 1)\n","\n","\n","def dense_knn_matrix(x, k=16, relative_pos=None):\n","    \"\"\"Get KNN based on the pairwise distance.\n","    Args:\n","        x: (batch_size, num_dims, num_points, 1)\n","        k: int\n","    Returns:\n","        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n","    \"\"\"\n","    with torch.no_grad():\n","        x = x.transpose(2, 1).squeeze(-1)\n","        batch_size, n_points, n_dims = x.shape\n","        ### memory efficient implementation ###\n","        n_part = 10000\n","        if n_points > n_part:\n","            nn_idx_list = []\n","            groups = math.ceil(n_points / n_part)\n","            for i in range(groups):\n","                start_idx = n_part * i\n","                end_idx = min(n_points, n_part * (i + 1))\n","                dist = part_pairwise_distance(x.detach(), start_idx, end_idx)\n","                if relative_pos is not None:\n","                    dist += relative_pos[:, start_idx:end_idx]\n","                _, nn_idx_part = torch.topk(-dist, k=k)\n","                nn_idx_list += [nn_idx_part]\n","            nn_idx = torch.cat(nn_idx_list, dim=1)\n","        else:\n","            dist = pairwise_distance(x.detach())\n","            if relative_pos is not None:\n","                dist += relative_pos\n","            _, nn_idx = torch.topk(-dist, k=k) # b, n, k\n","        ######\n","        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n","    return torch.stack((nn_idx, center_idx), dim=0)\n","\n","\n","def xy_dense_knn_matrix(x, y, k=16, relative_pos=None):\n","    \"\"\"Get KNN based on the pairwise distance.\n","    Args:\n","        x: (batch_size, num_dims, num_points, 1)\n","        k: int\n","    Returns:\n","        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n","    \"\"\"\n","    with torch.no_grad():\n","        x = x.transpose(2, 1).squeeze(-1)\n","        y = y.transpose(2, 1).squeeze(-1)\n","        batch_size, n_points, n_dims = x.shape\n","        dist = xy_pairwise_distance(x.detach(), y.detach())\n","        if relative_pos is not None:\n","            dist += relative_pos\n","        _, nn_idx = torch.topk(-dist, k=k)\n","        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n","    return torch.stack((nn_idx, center_idx), dim=0)\n","\n","\n","class DenseDilated(nn.Module):\n","    \"\"\"\n","    Find dilated neighbor from neighbor list\n","\n","    edge_index: (2, batch_size, num_points, k)\n","    \"\"\"\n","    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n","        super(DenseDilated, self).__init__()\n","        self.dilation = dilation\n","        self.stochastic = stochastic\n","        self.epsilon = epsilon\n","        self.k = k\n","\n","    def forward(self, edge_index):\n","        if self.stochastic:\n","            if torch.rand(1) < self.epsilon and self.training:\n","                num = self.k * self.dilation\n","                randnum = torch.randperm(num)[:self.k]\n","                edge_index = edge_index[:, :, :, randnum]\n","            else:\n","                edge_index = edge_index[:, :, :, ::self.dilation]\n","        else:\n","            edge_index = edge_index[:, :, :, ::self.dilation]\n","        return edge_index\n","\n","\n","class DenseDilatedKnnGraph(nn.Module):\n","    \"\"\"\n","    Find the neighbors' indices based on dilated knn\n","    \"\"\"\n","    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n","        super(DenseDilatedKnnGraph, self).__init__()\n","        self.dilation = dilation\n","        self.stochastic = stochastic\n","        self.epsilon = epsilon\n","        self.k = k\n","        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n","\n","    def forward(self, x, y=None, relative_pos=None):\n","        if y is not None:\n","            #### normalize\n","            x = F.normalize(x, p=2.0, dim=1)\n","            y = F.normalize(y, p=2.0, dim=1)\n","            ####\n","            edge_index = xy_dense_knn_matrix(x, y, self.k * self.dilation, relative_pos)\n","        else:\n","            #### normalize\n","            x = F.normalize(x, p=2.0, dim=1)\n","            ####\n","            edge_index = dense_knn_matrix(x, self.k * self.dilation, relative_pos)\n","        return self._dilated(edge_index)"],"metadata":{"id":"byvwp9BEvaAV","executionInfo":{"status":"aborted","timestamp":1660398714316,"user_tz":-540,"elapsed":462,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","import torch\n","from torch import nn\n","from torch.nn import Sequential as Seq, Linear as Lin, Conv2d\n","\n","\n","##############################\n","#    Basic layers\n","##############################\n","def act_layer(act, inplace=False, neg_slope=0.2, n_prelu=1):\n","    # activation layer\n","\n","    act = act.lower()\n","    if act == 'relu':\n","        layer = nn.ReLU(inplace)\n","    elif act == 'leakyrelu':\n","        layer = nn.LeakyReLU(neg_slope, inplace)\n","    elif act == 'prelu':\n","        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n","    elif act == 'gelu':\n","        layer = nn.GELU()\n","    elif act == 'hswish':\n","        layer = nn.Hardswish(inplace)\n","    else:\n","        raise NotImplementedError('activation layer [%s] is not found' % act)\n","    return layer\n","\n","\n","def norm_layer(norm, nc):\n","    # normalization layer 2d\n","    norm = norm.lower()\n","    if norm == 'batch':\n","        layer = nn.BatchNorm2d(nc, affine=True)\n","    elif norm == 'instance':\n","        layer = nn.InstanceNorm2d(nc, affine=False)\n","    else:\n","        raise NotImplementedError('normalization layer [%s] is not found' % norm)\n","    return layer\n","\n","\n","class MLP(Seq):\n","    def __init__(self, channels, act='relu', norm=None, bias=True):\n","        m = []\n","        for i in range(1, len(channels)):\n","            m.append(Lin(channels[i - 1], channels[i], bias))\n","            if act is not None and act.lower() != 'none':\n","                m.append(act_layer(act))\n","            if norm is not None and norm.lower() != 'none':\n","                m.append(norm_layer(norm, channels[-1]))\n","        super(MLP, self).__init__(*m)\n","\n","\n","class BasicConv(Seq):\n","    def __init__(self, channels, act='relu', norm=None, bias=True, drop=0.):\n","        m = []\n","        for i in range(1, len(channels)):\n","            m.append(Conv2d(channels[i - 1], channels[i], 1, bias=bias, groups=4))\n","            if norm is not None and norm.lower() != 'none':\n","                m.append(norm_layer(norm, channels[-1]))\n","            if act is not None and act.lower() != 'none':\n","                m.append(act_layer(act))\n","            if drop > 0:\n","                m.append(nn.Dropout2d(drop))\n","\n","        super(BasicConv, self).__init__(*m)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","\n","def batched_index_select(x, idx):\n","    r\"\"\"fetches neighbors features from a given neighbor idx\n","\n","    Args:\n","        x (Tensor): input feature Tensor\n","                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times 1}`.\n","        idx (Tensor): edge_idx\n","                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times l}`.\n","    Returns:\n","        Tensor: output neighbors features\n","            :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times k}`.\n","    \"\"\"\n","    batch_size, num_dims, num_vertices_reduced = x.shape[:3]\n","    _, num_vertices, k = idx.shape\n","    idx_base = torch.arange(0, batch_size, device=idx.device).view(-1, 1, 1) * num_vertices_reduced\n","    idx = idx + idx_base\n","    idx = idx.contiguous().view(-1)\n","\n","    x = x.transpose(2, 1)\n","    feature = x.contiguous().view(batch_size * num_vertices_reduced, -1)[idx, :]\n","    feature = feature.view(batch_size, num_vertices, k, num_dims).permute(0, 3, 1, 2).contiguous()\n","    return feature"],"metadata":{"id":"Bz3TActBvbaU","executionInfo":{"status":"aborted","timestamp":1660398714318,"user_tz":-540,"elapsed":464,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","import numpy as np\n","import torch\n","from torch import nn\n","# from .torch_nn import BasicConv, batched_index_select, act_layer\n","# from .torch_edge import DenseDilatedKnnGraph\n","# from .pos_embed import get_2d_relative_pos_embed\n","import torch.nn.functional as F\n","from timm.models.layers import DropPath\n","\n","\n","class MRConv2d(nn.Module):\n","    \"\"\"\n","    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(MRConv2d, self).__init__()\n","        self.nn = BasicConv([in_channels*2, out_channels], act, norm, bias)\n","\n","    def forward(self, x, edge_index, y=None):\n","        x_i = batched_index_select(x, edge_index[1])\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)\n","        b, c, n, _ = x.shape\n","        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2)], dim=2).reshape(b, 2 * c, n, _)\n","        return self.nn(x)\n","\n","\n","class EdgeConv2d(nn.Module):\n","    \"\"\"\n","    Edge convolution layer (with activation, batch normalization) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(EdgeConv2d, self).__init__()\n","        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)\n","\n","    def forward(self, x, edge_index, y=None):\n","        x_i = batched_index_select(x, edge_index[1])\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)\n","        return max_value\n","\n","\n","class GraphSAGE(nn.Module):\n","    \"\"\"\n","    GraphSAGE Graph Convolution (Paper: https://arxiv.org/abs/1706.02216) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(GraphSAGE, self).__init__()\n","        self.nn1 = BasicConv([in_channels, in_channels], act, norm, bias)\n","        self.nn2 = BasicConv([in_channels*2, out_channels], act, norm, bias)\n","\n","    def forward(self, x, edge_index, y=None):\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        x_j, _ = torch.max(self.nn1(x_j), -1, keepdim=True)\n","        return self.nn2(torch.cat([x, x_j], dim=1))\n","\n","\n","class GINConv2d(nn.Module):\n","    \"\"\"\n","    GIN Graph Convolution (Paper: https://arxiv.org/abs/1810.00826) for dense data type\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n","        super(GINConv2d, self).__init__()\n","        self.nn = BasicConv([in_channels, out_channels], act, norm, bias)\n","        eps_init = 0.0\n","        self.eps = nn.Parameter(torch.Tensor([eps_init]))\n","\n","    def forward(self, x, edge_index, y=None):\n","        if y is not None:\n","            x_j = batched_index_select(y, edge_index[0])\n","        else:\n","            x_j = batched_index_select(x, edge_index[0])\n","        x_j = torch.sum(x_j, -1, keepdim=True)\n","        return self.nn((1 + self.eps) * x + x_j)\n","\n","\n","class GraphConv2d(nn.Module):\n","    \"\"\"\n","    Static graph convolution layer\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, conv='edge', act='relu', norm=None, bias=True):\n","        super(GraphConv2d, self).__init__()\n","        if conv == 'edge':\n","            self.gconv = EdgeConv2d(in_channels, out_channels, act, norm, bias)\n","        elif conv == 'mr':\n","            self.gconv = MRConv2d(in_channels, out_channels, act, norm, bias)\n","        elif conv == 'sage':\n","            self.gconv = GraphSAGE(in_channels, out_channels, act, norm, bias)\n","        elif conv == 'gin':\n","            self.gconv = GINConv2d(in_channels, out_channels, act, norm, bias)\n","        else:\n","            raise NotImplementedError('conv:{} is not supported'.format(conv))\n","\n","    def forward(self, x, edge_index, y=None):\n","        return self.gconv(x, edge_index, y)\n","\n","\n","class DyGraphConv2d(GraphConv2d):\n","    \"\"\"\n","    Dynamic graph convolution layer\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=9, dilation=1, conv='edge', act='relu',\n","                 norm=None, bias=True, stochastic=False, epsilon=0.0, r=1):\n","        super(DyGraphConv2d, self).__init__(in_channels, out_channels, conv, act, norm, bias)\n","        self.k = kernel_size\n","        self.d = dilation\n","        self.r = r\n","        self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n","\n","    def forward(self, x, relative_pos=None):\n","        B, C, H, W = x.shape\n","        y = None\n","        if self.r > 1:\n","            y = F.avg_pool2d(x, self.r, self.r)\n","            y = y.reshape(B, C, -1, 1).contiguous()            \n","        x = x.reshape(B, C, -1, 1).contiguous()\n","        edge_index = self.dilated_knn_graph(x, y, relative_pos)\n","        x = super(DyGraphConv2d, self).forward(x, edge_index, y)\n","        return x.reshape(B, -1, H, W).contiguous()\n","\n","\n","class Grapher(nn.Module):\n","    \"\"\"\n","    Grapher module with graph convolution and fc layers\n","    \"\"\"\n","    def __init__(self, in_channels, kernel_size=9, dilation=1, conv='edge', act='relu', norm=None,\n","                 bias=True,  stochastic=False, epsilon=0.0, r=1, n=196, drop_path=0.0, relative_pos=False):\n","        super(Grapher, self).__init__()\n","        self.channels = in_channels\n","        self.n = n\n","        self.r = r\n","        self.fc1 = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(in_channels),\n","        )\n","        self.graph_conv = DyGraphConv2d(in_channels, in_channels * 2, kernel_size, dilation, conv,\n","                              act, norm, bias, stochastic, epsilon, r)\n","        self.fc2 = nn.Sequential(\n","            nn.Conv2d(in_channels * 2, in_channels, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(in_channels),\n","        )\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.relative_pos = None\n","        if relative_pos:\n","            print('using relative_pos')\n","            relative_pos_tensor = torch.from_numpy(np.float32(get_2d_relative_pos_embed(in_channels,\n","                int(n**0.5)))).unsqueeze(0).unsqueeze(1)\n","            relative_pos_tensor = F.interpolate(\n","                    relative_pos_tensor, size=(n, n//(r*r)), mode='bicubic', align_corners=False)\n","            self.relative_pos = nn.Parameter(-relative_pos_tensor.squeeze(1), requires_grad=False)\n","\n","    def _get_relative_pos(self, relative_pos, H, W):\n","        if relative_pos is None or H * W == self.n:\n","            return relative_pos\n","        else:\n","            N = H * W\n","            N_reduced = N // (self.r * self.r)\n","            return F.interpolate(relative_pos.unsqueeze(0), size=(N, N_reduced), mode=\"bicubic\").squeeze(0)\n","\n","    def forward(self, x):\n","        _tmp = x\n","        x = self.fc1(x)\n","        B, C, H, W = x.shape\n","        relative_pos = self._get_relative_pos(self.relative_pos, H, W)\n","        x = self.graph_conv(x, relative_pos)\n","        x = self.fc2(x)\n","        x = self.drop_path(x) + _tmp\n","        return x"],"metadata":{"id":"1mr_H8cQvcaU","executionInfo":{"status":"aborted","timestamp":1660398714319,"user_tz":-540,"elapsed":12,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 2022.06.17-Changed for building ViG model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential as Seq\n","\n","from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","from timm.models.helpers import load_pretrained\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.registry import register_model\n","\n","# from gcn_lib import Grapher, act_layer\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n","        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n","        **kwargs\n","    }\n","\n","\n","default_cfgs = {\n","    'vig_224_gelu': _cfg(\n","        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n","    ),\n","    'vig_b_224_gelu': _cfg(\n","        crop_pct=0.95, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n","    ),\n","}\n","\n","\n","class FFN(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act='relu', drop_path=0.0):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Sequential(\n","            nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(hidden_features),\n","        )\n","        self.act = act_layer(act)\n","        self.fc2 = nn.Sequential(\n","            nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0),\n","            nn.BatchNorm2d(out_features),\n","        )\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\n","    def forward(self, x):\n","        shortcut = x\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.fc2(x)\n","        x = self.drop_path(x) + shortcut\n","        return x#.reshape(B, C, N, 1)\n","\n","\n","class Stem(nn.Module):\n","    \"\"\" Image to Visual Embedding\n","    Overlap: https://arxiv.org/pdf/2106.13797.pdf\n","    \"\"\"\n","    def __init__(self, img_size=224, in_dim=3, out_dim=768, act='relu'):\n","        super().__init__()        \n","        self.convs = nn.Sequential(\n","            nn.Conv2d(in_dim, out_dim//2, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim//2),\n","            act_layer(act),\n","            nn.Conv2d(out_dim//2, out_dim, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","            act_layer(act),\n","            nn.Conv2d(out_dim, out_dim, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        return x\n","\n","\n","class Downsample(nn.Module):\n","    \"\"\" Convolution-based downsample\n","    \"\"\"\n","    def __init__(self, in_dim=3, out_dim=768):\n","        super().__init__()        \n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class DeepGCN(torch.nn.Module):\n","    def __init__(self, opt):\n","        super(DeepGCN, self).__init__()\n","        print(opt)\n","        k = opt.k\n","        act = opt.act\n","        norm = opt.norm\n","        bias = opt.bias\n","        epsilon = opt.epsilon\n","        stochastic = opt.use_stochastic\n","        conv = opt.conv\n","        emb_dims = opt.emb_dims\n","        drop_path = opt.drop_path\n","        \n","        blocks = opt.blocks\n","        self.n_blocks = sum(blocks)\n","        channels = opt.channels\n","        reduce_ratios = [4, 2, 1, 1]\n","        dpr = [x.item() for x in torch.linspace(0, drop_path, self.n_blocks)]  # stochastic depth decay rule \n","        num_knn = [int(x.item()) for x in torch.linspace(k, k, self.n_blocks)]  # number of knn's k\n","        max_dilation = 49 // max(num_knn)\n","        \n","        self.stem = Stem(out_dim=channels[0], act=act)\n","        self.pos_embed = nn.Parameter(torch.zeros(1, channels[0], 224//4, 224//4))\n","        HW = 224 // 4 * 224 // 4\n","\n","        self.backbone = nn.ModuleList([])\n","        idx = 0\n","        for i in range(len(blocks)):\n","            if i > 0:\n","                self.backbone.append(Downsample(channels[i-1], channels[i]))\n","                HW = HW // 4\n","            for j in range(blocks[i]):\n","                self.backbone += [\n","                    Seq(Grapher(channels[i], num_knn[idx], min(idx // 4 + 1, max_dilation), conv, act, norm,\n","                                    bias, stochastic, epsilon, reduce_ratios[i], n=HW, drop_path=dpr[idx],\n","                                    relative_pos=True),\n","                          FFN(channels[i], channels[i] * 4, act=act, drop_path=dpr[idx])\n","                         )]\n","                idx += 1\n","        self.backbone = Seq(*self.backbone)\n","\n","        self.prediction = Seq(nn.Conv2d(channels[-1], 1024, 1, bias=True),\n","                              nn.BatchNorm2d(1024),\n","                              act_layer(act),\n","                              nn.Dropout(opt.dropout),\n","                              nn.Conv2d(1024, opt.n_classes, 1, bias=True))\n","        self.model_init()\n","\n","    def model_init(self):\n","        for m in self.modules():\n","            if isinstance(m, torch.nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","                m.weight.requires_grad = True\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","                    m.bias.requires_grad = True\n","\n","    def forward(self, inputs):\n","        x = self.stem(inputs) + self.pos_embed\n","        B, C, H, W = x.shape\n","        for i in range(len(self.backbone)):\n","            x = self.backbone[i](x)\n","\n","        x = F.adaptive_avg_pool2d(x, 1)\n","        return self.prediction(x).squeeze(-1).squeeze(-1)\n","\n","\n","@register_model\n","def pvig_ti_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n","            self.channels = [48, 96, 240, 384] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_224_gelu']\n","    return model\n","\n","\n","@register_model\n","def pvig_s_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n","            self.channels = [80, 160, 400, 640] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_224_gelu']\n","    return model\n","\n","\n","@register_model\n","def pvig_m_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,16,2] # number of basic blocks in the backbone\n","            self.channels = [96, 192, 384, 768] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_224_gelu']\n","    return model\n","\n","\n","@register_model\n","def pvig_b_224(pretrained=False, **kwargs):\n","    class OptInit:\n","        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n","            self.k = 9 # neighbor num (default:9)\n","            self.conv = 'mr' # graph conv layer {edge, mr}\n","            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n","            self.norm = 'batch' # batch or instance normalization {batch, instance}\n","            self.bias = True # bias of conv layer True or False\n","            self.dropout = 0.0 # dropout rate\n","            self.use_dilation = True # use dilated knn or not\n","            self.epsilon = 0.2 # stochastic epsilon for gcn\n","            self.use_stochastic = False # stochastic for gcn, True or False\n","            self.drop_path = drop_path_rate\n","            self.blocks = [2,2,18,2] # number of basic blocks in the backbone\n","            self.channels = [128, 256, 512, 1024] # number of channels of deep features\n","            self.n_classes = num_classes # Dimension of out_channels\n","            self.emb_dims = 1024 # Dimension of embeddings\n","\n","    opt = OptInit(**kwargs)\n","    model = DeepGCN(opt)\n","    model.default_cfg = default_cfgs['vig_b_224_gelu']\n","    return model"],"metadata":{"id":"jJ--cYv1vdXE","executionInfo":{"status":"aborted","timestamp":1660398714319,"user_tz":-540,"elapsed":12,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 초기화, 옵티마이저 정의 등등"],"metadata":{"id":"nVi8eZCUZvPT"}},{"cell_type":"code","source":["model = pvig_b_224()\n","model = model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n","criterion = nn.CrossEntropyLoss()\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"],"metadata":{"id":"MFqICJoPvlHt","executionInfo":{"status":"aborted","timestamp":1660398714320,"user_tz":-540,"elapsed":12,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.load_state_dict(model_state_dict)"],"metadata":{"id":"VrmRdAZLECqI","executionInfo":{"status":"aborted","timestamp":1660398714321,"user_tz":-540,"elapsed":13,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for param_tensor in model.state_dict():\n","  print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"],"metadata":{"id":"ClkP0Tg_zApY","executionInfo":{"status":"aborted","timestamp":1660398656892,"user_tz":-540,"elapsed":22,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for var_name in optimizer.state_dict():\n","  print(var_name, \"\\t\", optimizer.state_dict()[var_name])"],"metadata":{"id":"VN2NA1FU1FbO","executionInfo":{"status":"aborted","timestamp":1660398656892,"user_tz":-540,"elapsed":22,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","\n","summary(model, (3,224,224))"],"metadata":{"id":"yNrO9OOyvnUc","executionInfo":{"status":"aborted","timestamp":1660398714321,"user_tz":-540,"elapsed":13,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 01. FruitAndVegetable 데이터셋 실험"],"metadata":{"id":"Ly9jwSovZ6JW"}},{"cell_type":"code","source":["# 학습을 위해 데이터 증가(augmentation) 및 일반화(normalization)\n","# 검증을 위한 일반화\n","# from email.mime import image\n","\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","\n","data_dir = '/content/drive/MyDrive/Data/FruitAndVegetables/'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","print(len(image_datasets['train']))\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n","                                             shuffle=True, num_workers=2)\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"OKrwyh88vrMd","executionInfo":{"status":"aborted","timestamp":1660398656894,"user_tz":-540,"elapsed":24,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transform 된 배치 이미지 시각화\n","def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # 갱신이 될 때까지 잠시 기다립니다.\n","\n","\n","# 학습 데이터의 배치를 얻습니다.\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","# 배치로부터 격자 형태의 이미지를 만듭니다.\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out)\n","print([class_names[x] for x in classes])"],"metadata":{"id":"8zRk7Byhwnol","executionInfo":{"status":"aborted","timestamp":1660398656894,"user_tz":-540,"elapsed":24,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","def train_model(model, criterion, optimizer, scheduler, num_epochs):\n","    model = model.to(device)\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in tqdm(range(num_epochs)):\n","        \n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print('-' * 10)\n","        # print(f\"time per 1 epoch: {time.time() - since:.2f}\")\n","\n","        # 각 에폭(epoch)은 학습 단계와 검증 단계를 갖습니다.\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                # model.load_state_dict(model_state_dict)\n","                model.train()  # 모델을 학습 모드로 설정\n","            else:\n","                # model.load_state_dict(model_state_dict)\n","                model.eval()   # 모델을 평가 모드로 설정\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # 데이터를 반복\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # 매개변수 경사도를 0으로 설정\n","                optimizer.zero_grad()\n","\n","                # 순전파\n","                # 학습 시에만 연산 기록을 추적\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # 학습 단계인 경우 역전파 + 최적화\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # 통계\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","            \n","            # wandb.log({'train_acc': epoch_acc, 'train_loss' : epoch_loss})\n","\n","            # 모델을 깊은 복사(deep copy)함\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","        time.sleep(0.1)\n","\n","    time_elapsed = time.time() - since\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best val Acc: {best_acc:4f}')\n","    # wandb.log({'test_acc': best_acc})\n","    \n","\n","    # 가장 나은 모델 가중치를 불러옴\n","    model.load_state_dict(best_model_wts)\n","    return model"],"metadata":{"id":"sUmm5qjeENjm","executionInfo":{"status":"aborted","timestamp":1660398656895,"user_tz":-540,"elapsed":25,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"rbKtbgLIAYNQ","executionInfo":{"status":"aborted","timestamp":1660398656895,"user_tz":-540,"elapsed":25,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_model(model, criterion, optimizer, exp_lr_scheduler,\n","                       num_epochs=20)"],"metadata":{"id":"WN98LcHyERWr","executionInfo":{"status":"aborted","timestamp":1660398656896,"user_tz":-540,"elapsed":26,"user":{"displayName":"JiHun Bae","userId":"17761386967035840493"}}},"execution_count":null,"outputs":[]}]}